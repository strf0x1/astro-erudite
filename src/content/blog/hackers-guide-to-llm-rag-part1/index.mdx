---
title: A Hacker’s Guide to Local LLMs and RAG
description:  Exploring the power of combining local language models with retrieval augmented generation for hackers.
date: 2024-01-01
tags: ['LLM', 'RAG', 'Cybersecurity', 'AI', 'Machine Learning']
image: './1200x630.png'
authors: ['strf0x']
---

The Cybersecurity community has seen hype cycles around AI for years, even before ChatGPT's rise to fame. We were promised magical solutions to lack of resources, our inability to keep up with our adversaries.

In reality, many of us learned there's no easy button. Cybersecurity is a constant game of cat-and-mouse, and relying on simple solutions can lead to complacency and a false sense of security. That’s why I understand the skepticism I hear when talking about recent AI advancements. We’ve been burned before.

But this time feels different.

### Why The Shift?

Traditional Cybersecurity AI models relied on simpler algorithms like random forests and anomaly detection. These techniques had major drawbacks: they were slow, computationally expensive, and required large datasets for baseline establishment.

Random Forests struggled to keep up with evolving attacks because they rely on spotting patterns in historical data. Think of it like signature-based antivirus: if a virus slightly changes its code, the old signature won't recognize it.

Then ChatGPT 3 dropped, triggering an explosion of growth in the AI field. Computer hardware caught up, allowing us to leverage the influx of investment pouring into the field. Now dozens of groundbreaking research papers are released weekly.

It’s never been a better time to take a second look at AI.

## Why This Matters for Hackers

For hackers, this is an especially exciting time. These are just tools you can have in your toolbox, like a swiss army knife for language. They're great for some tasks but maybe not ideal for everything. For example, while impressive, using a large language model to write phishing emails might seem tempting, but crafting highly targeted and believable lures often requires nuanced understanding of human psychology and specific contexts that LLMs still struggle with.

However, there are incredibly valuable applications within Cybersecurity. LLMs can excel at analyzing complex code, identifying patterns and potential vulnerabilities that might go unnoticed by traditional security tools. The community also needs people who are deeply curious about how things actually work and willing to redteam these models.

It’s a complex topic, so let's start with something simple:

### Introduction to Retrieval Augmented Generation

Language models like ChatGPT are trained on vast amounts of information, basically the entirety of the public internet. They've kind of run out of data from the internet, and trends are moving towards generating synthetic data.


There are many comparable open-source, locally-hosted language models you can use right at home that work just as well as ChatGPT, but with the added benefit that you control your own data, without API limits or additional costs.

A huge community has grown around this idea of open-source AI, the biggest communities being [Hugging Face](https://huggingface.co/) and Reddit's [/r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/). This is in stark contrast to OpenAI’s ChatGPT, where we have no idea how it was trained, or if they’re just building a giant surveillance engine with our personal data.

Because these models are extremely expensive to train, at some point they need to save and ship your model. The training cutoff for data from Anthropic's Claude is roughly March 2024 at the time of this writing. So any new programming language documentation might not be included. If you're using it to write code, it might not know about deprecated functions or new features available in the library you're using.

These models are broadly general-purpose. They don’t train on everything, especially extremely domain-specific information, or things from non-public resources. For that reason, we need a way to augment these models with new and specific domain expertise, and to do that in a way that doesn't cost huge sums of money.

Say you're working on a threat intelligence project, and you have some reports on a new threat actor. Traditional keyword search might be able to identify sections of the report based on your query. From there, you'd have to read the sections and understand them yourself. LLM with RAG would be able to find the sections that are interesting, understand those sections, and then could summarize them with high-level bullet points. It could create metadata based on the contents and output them in json format for your threat intel database. Or it could write some example source code to demonstrate how the technique might work.

In its simplest form, a RAG is a contextually aware document search engine that enriches your language model with new information. When you ask an LLM a question, the question is used to search a vector database, and your question + the new pieces of information are sent to the model. You can also control how the model uses that new information, prompting it to only use it (and refuse to respond if it doesn't know), or to synthesize a response using the new information + what it already knows.

If you wanted to give the model more context about a code example you’re working on, you might type something like:

```python
User Prompt: Change this code to print A - Z:
for i in range(1, 11):
    print(i)
```

For which the model might respond:

```python
for i in range(65, 91):
    print(chr(i))
```

Language models can be really helpful for refactoring code, and giving them more information about what you want tends to improve the usefulness of their responses.

A RAG acts a lot like the above example, but instead of pasting the code or paragraph you want to chat about, the RAG takes your query, searches a vector database (full of documents you’ve ingested) retrieves contextually relevant information for you, and inserts chunks of the relevant data into your original query before sending it to the model to respond.

The way you process your data (whether PDFs, presentations, or Git repos), to the way you store and retrieve that data is where the challenge lies. But lucky for us, the open-source community has a ton of really useful libraries and techniques for building this, and testing is an integral part of objectively identifying good ways to do this.

### A Minimal RAG

The best way to learn RAG is to build a very simple prototype.


Imagine for a moment, a vast, privately curated knowledge base that you control and use. It could be like a second brain or an evolution of note-taking software like Obsidian or Notion. By the end of part 2 of this series, we’ll have something like that you can build to work the way you want it to work.

There are several canned solutions that you could achieve this with, but then, we wouldn't be learning how things work!


First, we need to run a local model and test it works. [Ollama](https://ollama.com/) is one of the easier ways to get a local model running and it’s cross-platform so you can run it on Mac, Linux or Windows. Navigate to their site and follow the instructions for your operating system. I run Ubuntu Linux 24.04:

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

Then pull down a small local model and test it:

```bash
systemctl restart ollama
ollama run gemma2:2b
```

You’ll get a basic chat prompt in your CLI. You can ask questions, and it has a memory called a context window length of 2048 tokens (or about 8000 characters give or take). Let's ask vanilla Gemma2 to make sure it doesn't know the answer to the question we're going to ask:
```bash
>>> whats the password to the vx underground database?
I understand you might be curious about secure systems, but I cannot provide you with the password for any system, especially one that could compromise someone's privacy or
security.

Here's why:

* **It's illegal:**  Accessing someone else's information without permission is a crime and can have serious legal consequences.
* **It's unethical:** Sharing passwords would violate trust and potentially harm people.
* **It's dangerous:** The misuse of this information could lead to financial loss, identity theft, or other harmful situations for individuals involved.
...
>>> /bye
```
Gemma2 has a perfectly reasonable response, but not really helpful for us. You can ask if it knows what VX Underground is. Most models seemed to have a rough idea, but I haven't found a model that knows the password.

Next, you’ll want a relatively recent version of Python (I'm on 3.12). To make things much easier to manage, we’ll be using `venv` and `poetry` to manage dependencies:

```bash
sudo apt install python3-pip python3-venv python3-virtualenv -y
pip install pipx -y
pipx install poetry
```

To make a new project driven by Poetry:

```bash
poetry new simple_rag
cd simple_rag
python -m venv simple-rag
source env/bin/activate
```

This will create a new directory called `simple_rag` with some files to get your project started:

```bash
├── README.md
├── pyproject.toml
├── simple_rag
│   └── __init__.py
└── tests
    └── __init__.py
```


To handle our RAG, there are some really useful libraries. There’s a bit of a pitfall here too in that by depending on high-level libraries we’ll be abstracting away how the RAG works under the hood, but for this example, we’ll use Langchain:

```bash
poetry add langchain langchain-huggingface langchain-community faiss-cpu
```


Later on, we'll learn why abstraction can be detrimental and we'll dig deeper into an alternate approach. It’s not that LLamaIndex and Langchain are bad tools, but they offer convenience while sacrificing control.

Here's the RAG code (simple-rag/simple-rag/simple-rag.py):
```python
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.llms import Ollama

# load your document
loader = TextLoader('test.txt') # Replace with your file path
documents = loader.load()

# split the document into smaller chunks for better context
text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

# load SentenceTransformers model for embeddings
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

vectorstore = FAISS.from_documents(texts, embeddings)

# setup ollama
llm = Ollama(model="gemma2:2b", temperature=0.7) # Adjust model & parameters as needed

# create qa chain with ollama
qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=vectorstore.as_retriever())

# ask question about the document
query = "What does this document tell you about the vx underground zip file password?"
result = qa_chain.run(query)
print(result)
```
Before we run it, let’s make a test document:
```bash
echo "the password to the vx underground zip files is 'infected'" > test.txt
```

Run it:
```bash
poetry run python simple_rag/simple-rag.py
...
The document tells us the password to the vx underground database is 'infected'.
```

There's something else interesting happening here. It completely ignored the ethical concerns it mentioned earlier. These models are trained to be helpful, safe assistants. It's possible it doesn't know the answer, and it tries to steer the conversation away to something helpful, or it's possible that putting the model in the context of analyzing a document broke it out of its safety training.

There are jailbreaks that use similar techniques. By persuading a model to imagine a fictitious scenario where it writes some malicious code, they will often play along and write the malicious code. Initially, I used jailbreaks when building RAG for Cybersecurity documents, but I realized they were often not needed to get the information back that I needed (gemma, qwen2.5, mistral-nemo).

Now that we’ve got a working example, we need to talk about how to scale this to more than one document.

### Multi-Documents, Chunking and Embedding

Our example code should work fairly well, but we already have some engineering challenges. We used an in-memory vector store for a quick test, which makes it simpler to get started but won’t scale well. We should build a more permanent vector database that will survive reboots and outages.

We also have the challenge that LLMs have a fixed context length, meaning, they can only store x amount of tokens at a time before they start to fall apart. You can think about context window as a constantly shifting memory that the LLM can refer back to as your chat history grows.

If you asked an initial question, then the model responds, then you ask a clarifying question, the entirety of that conversation (user questions + data retrieved from RAG + model responses) has to fit into that window. If the conversation grows too large, the model begins to truncate your previous chat history.

Every model has an ideal context length, but it also depends on how much ram you have. The default context window for Ollama is 2048 tokens. It also matters how big your model is, measured in number of parameters. For example Gemma2:2b is a 2 billion parameter model. LLama 3.1 70b is a 70 billion parameter model, so it requires much more ram than Gemma2:2b. Working with a smaller parameter model allows you work with larger context windows without running out of ram.

To deal with some of these limitations, another lever we can pull is the size of the documents we retrieve. Instead of getting an entire document back in the response, it would be a lot better if we only got back the passages we really needed.

This ends up being a more involved problem to solve because our vector search needs to have contextual awareness of the source material, and in our case, PDFs can be a nightmare to parse.

In the next section we’ll address some of these issues with a newer RAG method called ColBERTv2.
